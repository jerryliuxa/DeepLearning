{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryliuxa/DeepLearning/blob/master/ChatGLM3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGLM3是智谱AI和清华大学KEG实验室联合发布的新一代对话预训练模型\n",
        "\n",
        "原项目地址：https://github.com/THUDM/ChatGLM3\n",
        "\n",
        "此Colab笔记本由Kihh制作编写\n",
        "\n",
        "博文：https://blog.kihh.xyz/p/在google-colab快速部署chatglm3-6b/"
      ],
      "metadata": {
        "id": "z-OgssbBwoZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t3hTuFWVbCw0",
        "outputId": "3d6049c6-d6c1-4819-99db-389a53c12bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr  9 07:35:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#检查显卡环境\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#环境部署\n",
        "!git clone https://github.com/THUDM/ChatGLM3\n",
        "%cd ChatGLM3\n",
        "!pip install --upgrade pip\n",
        "!pip install protobuf 'transformers>=4.30.2' cpm_kernels 'torch>=2.0' gradio mdtex2html sentencepiece accelerate\n",
        "!pip install peft"
      ],
      "metadata": {
        "id": "lV4cW1XXbKbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#部署cloudflared内网穿透\n",
        "!curl -Lo /usr/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x /usr/bin/cloudflared"
      ],
      "metadata": {
        "id": "sal-ArD4nWBJ",
        "outputId": "ee3ef8e9-bcaf-42c1-bf50-e9c77e66e5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 34.8M  100 34.8M    0     0  30.2M      0  0:00:01  0:00:01 --:--:-- 83.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Modle files\n",
        "! git clone https://huggingface.co/THUDM/chatglm3-6b/"
      ],
      "metadata": {
        "id": "k6_-fbfv1DcS",
        "outputId": "67d97ef7-23d1-4d1f-fd61-2796e61ea5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatglm3-6b'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 128 (delta 62), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (128/128), 51.37 KiB | 17.12 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#运行ChatGLM3 Gradio网页示例\n",
        "!python /content/ChatGLM3/basic_demo/web_demo_gradio.py & cloudflared tunnel --url localhost:8501"
      ],
      "metadata": {
        "id": "WCLJ7qY0jYAI",
        "outputId": "5b1463ea-7399-4dfb-9b95-f43751ee5aea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2024-04-09T07:47:14Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2024-04-09T07:47:14Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m |  https://opens-speak-y-friend.trycloudflare.com                                            |\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m Version 2024.3.0\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.21.5, GoArch: amd64\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:localhost:8501]\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 659f364f-3a06-4eb2-b838-69b045f2ddb6\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/run-tunnel/as-a-service/\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:43173/metrics\n",
            "2024/04/09 07:47:16 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2024-04-09T07:47:16Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m52828bd8-41fc-43b0-8b68-6f966154a510 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167 \u001b[36mlocation=\u001b[0msea01 \u001b[36mprotocol=\u001b[0mquic\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/ChatGLM3/THUDM/chatglm3-6b'. Use `repo_type` argument if needed.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ChatGLM3/basic_demo/web_demo_gradio.py\", line 69, in <module>\n",
            "    model, tokenizer = load_model_and_tokenizer(MODEL_PATH, trust_remote_code=True)\n",
            "  File \"/content/ChatGLM3/basic_demo/web_demo_gradio.py\", line 59, in load_model_and_tokenizer\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 462, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: Incorrect path_or_model_id: '/content/ChatGLM3/THUDM/chatglm3-6b'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n",
            "\u001b[90m2024-04-09T07:47:25Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:8501: connect: connection refused\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2024-04-09T07:47:25Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:8501: connect: connection refused\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://opens-speak-y-friend.trycloudflare.com/ \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2024-04-09T07:47:27Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:8501: connect: connection refused\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://localhost:8501\n",
            "\u001b[90m2024-04-09T07:47:27Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared: dial tcp 127.0.0.1:8501: connect: connection refused\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://opens-speak-y-friend.trycloudflare.com/favicon.ico \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167 \u001b[36mtype=\u001b[0mhttp\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[32mINF\u001b[0m Unregistered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve quic connection \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m icmp router terminated \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m\n",
            "\u001b[90m2024-04-09T07:47:50Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "6QuFyJEL0rvD",
        "outputId": "0563f7e2-67b7-4cfe-ad72-8d4ac95e6932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft\n",
            "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.10.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-X48dv90wFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}